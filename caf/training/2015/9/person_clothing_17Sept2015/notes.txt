17 Sept. 2015
Training person_clothes_17Sept2015
Using the bvlc reference caffe net structure, at first with rotation contraint for conv1-3 (later give this up.)

Check learning rate relative to Barry Lyndon fine tuning:

I0917 10:52:24.812410 24804 solver.cpp:189] Iteration 28500, loss = 0.205567
I0917 11:05:45.031268 24804 solver.cpp:189] Iteration 28600, loss = 0.174446
100 iterations in ~13 minutes without image resizing within caffe


I0917 12:29:46.223680 31332 solver.cpp:189] Iteration 0, loss = 7.36798
I0917 12:49:50.695200 31332 solver.cpp:189] Iteration 100, loss = 6.84288
100 iterations in ~20 minutes with image resizing within caffe

I0917 12:49:50.695200 31332 solver.cpp:189] Iteration 100, loss = 6.84288
I0917 13:05:04.328654 31332 solver.cpp:189] Iteration 200, loss = 6.82504
100 iterations in ~15 minutes with image resizing within caffe


I0917 13:18:51.232166 31332 solver.cpp:189] Iteration 300, loss = 6.80337
I0917 13:31:37.153866 31332 solver.cpp:189] Iteration 400, loss = 6.82383
100 iterations in ~13 minutes with image resizing within caffe


Thus, at first glance it seems that loading jpgs on the fly and resizing is not much or at all worse than not resizing.


I first trained a model for 600 iterations with the rotational convolution constraint. It stopped because a jpeg failed to load.
Because training was slow, I started over without this constraint. I moved the first set of weights into conv_rot_attempt/

Learning seems to be faster without the conv_rot constraint (at 600 iterations).